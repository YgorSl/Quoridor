import numpy as np
import random
from copy import copy
import minmax

# Now we can define the MDP as a tuple (S, A, T, R, ùõæ).
  # Here, R(s, a) is the reward for taking action a in state s, P(s'|s, a) is the transition probability of reaching state s' given state s and action a, and ùõæ is the discount factor.
class Qlearning:
  def __init__(
    self,
    problema,
    desconto = 0.90,
    tetha = 1e-6,
    alpha = 0.1
  ):
    self.problema = problema
    self.n_estados = len(problema.tabuleiro)
    self.n_acoes = len(problema.acoes)
    self.theta = tetha
    self.alpha = alpha
    self.desconto = desconto
    self.e = 0.4
    
    self.Q = np.zeros((self.n_estados, self.n_acoes))
    self.PI = np.zeros(self.n_estados, dtype=int) 

  def calcular_tabela_q(self, estado_inicial = 0, n_passos = 10000, limite_max = 10):
    passo = 0
    
    while passo < n_passos:
      if (passo % 10000 == 0): print("%s passos de %s" %(passo, n_passos))
      passo += 1
      novo_problema = copy(self.problema)#estado √© a posi√ß√£o
      # o meu tem que ser o tabuleiro
      #estado = estado_inicial # estado inicial
      estado = self.problema.tabuleiro
      limite = 0
      while self.problema.is_end() == False and limite < limite_max:
        limite += 1
        # escolha da acao
        # random ou melhor da pol√≠tica baseado em uma taxa
        acao = self.sorteia_proxima_acao(estado)
        
        q_antigo = self.Q[estado][acao]
        q_seguinte = 0
        
        # Atualiza√ß√£o da Q Table
        # Q(s,a) <= Œ±(*Q(s,a) + (1-Œ±)*Œ£(s')( T(s, a, s') * [ R(s,a,s') +  ùõæ max(a) Q(s',a') ] )
        # Q(s,a) <= Œ±(*Q(s,a) + (1-Œ±)*amostra)
        
        # amostra = Œ£(s') T(s, a, s') * [ R(s,a,s') +  ùõæ max(a) Q(s',a')
        for proximo_estado, probabilidade in self.problema.T(estado, acao):
          q_seguinte += self.novo_q(estado, acao, proximo_estado, probabilidade)
        # aqui acontece a atualiza√ß√£o do Q(s,a)
        self.Q[estado][acao] = self.alpha * q_antigo + (1 - self.alpha) * q_seguinte
  
        # Itera√ß√£o por pol√≠tica 
        # pega melhor a√ß√£o para o estado S
        self.PI[estado] = np.argmax(self.Q[estado])
        
        # escolhe o proximo estado probabilisticamente
        estado = self.sorteia_proximo_estado(estado, acao)
        jogo_prox = copy(self.problema)
        jogo_prox.tabuleiro = estado
        proxima_jogada = minmax.melhor_jogada(jogo_prox)
        self.problema.aplicar_acao(proxima_jogada, "A")
        estado = self.problema.tabuleiro
        
      self.problema = novo_problema


#se eu alterar num novo problema as fun√ß√µes q e T estar√£o calculando do problema da classe
    return self.Q, self.PI

  def novo_q(self, estado, acao, proximo_estado, probabilidade):
    max_a = np.max(self.Q[proximo_estado]) # max(a) Q(s',a')
    return probabilidade * (self.problema.R(estado, acao, proximo_estado) + self.desconto * max_a)
  
  # escolha da acao
  # random ou melhor da pol√≠tica baseado em uma taxa
  def sorteia_proxima_acao(self, estado):
    acao_random = random.randrange(self.n_acoes)
    acao_politica = self.PI[estado]
    return random.choices([acao_random, acao_politica], weights = [self.e, (1-self.e)])[0]

  # dado um estado e a√ß√£o
  # sorteia o pr√≥ximo estado baseado 
  # nas suas probabilidades de T(s,a, s')
  def sorteia_proximo_estado(self, estado, acao):
    prox_estados = self.problema.T(estado, acao)
    
    estados = []
    probs = []
    for (prox_estado, prob) in prox_estados:
      estados.append(prox_estado)
      probs.append(prob)
    #https://acervolima.com/metodo-random-choices-em-python/
    return random.choices(estados, weights=probs)[0]